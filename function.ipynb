{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0793ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db85d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_standard_ppl_with_sliding_window(model, tokenizer, dataset, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    结合了：\n",
    "    1. 滑动窗口 (Stride) -> 保证每个 token 都有足够的上文\n",
    "    2. 全局聚合 (Global Aggregation) -> 符合 PPL 标准定义\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. 拼接全量文本\n",
    "    encodings = tokenizer(\"\\n\\n\".join(dataset[\"text\"]), return_tensors=\"pt\")\n",
    "    \n",
    "    max_length = model.config.n_positions\n",
    "    stride = 512  # 滑动步长，通常设为 max_length 的一半或更小\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "    nlls = []\n",
    "    prev_end_loc = 0\n",
    "    \n",
    "    # tqdm 进度条\n",
    "    for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # 我们这一轮实际要评估的 token 长度\n",
    "        \n",
    "        # 获取当前窗口的 input_ids\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        \n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100 \n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            \n",
    "            # outputs.loss 是平均 loss，我们需要还原成 sum loss\n",
    "            # 因为最后一个 batch 的 trg_len 可能不等于 stride\n",
    "            neg_log_likelihood = outputs.loss * trg_len\n",
    "\n",
    "        nlls.append(neg_log_likelihood)\n",
    "        \n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "\n",
    "    # 3. 全局计算 PPL\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / seq_len) # 总 NLL / 总长度\n",
    "    return ppl.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1745be96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    \"\"\"将文本拼接并切块 (Packing)\"\"\"\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    \n",
    "    if total_length >= MAX_LENGTH:\n",
    "        total_length = (total_length // MAX_LENGTH) * MAX_LENGTH\n",
    "    \n",
    "    result = {\n",
    "        k: [t[i : i + MAX_LENGTH] for i in range(0, total_length, MAX_LENGTH)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "def train_model(model, train_dataset, eval_dataset, tokenizer, output_dir):\n",
    "    \"\"\"\n",
    "    模型微调函数：包含自动列清理、BF16加速、Epoch级评估\n",
    "    \"\"\"\n",
    "    # 内部预处理函数\n",
    "    def preprocess_dataset(dataset):\n",
    "        # 关键：获取当前所有列名 (text, entropy 等)，以便稍后移除\n",
    "        column_names = dataset.column_names\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            # 批量处理：给每个文本末尾加上 EOS\n",
    "            return tokenizer(\n",
    "                [t + tokenizer.eos_token for t in examples[\"text\"]]\n",
    "            )\n",
    "        \n",
    "        # 1. Tokenize 并移除旧列\n",
    "        tokenized = dataset.map(\n",
    "            tokenize_function, \n",
    "            batched=True, \n",
    "            num_proc=8,\n",
    "            remove_columns=column_names # 彻底清理，防止 group_texts 报错\n",
    "        )\n",
    "        \n",
    "        # 2. Packing\n",
    "        packed = tokenized.map(\n",
    "            group_texts, \n",
    "            batched=True,\n",
    "            num_proc=8\n",
    "        )\n",
    "        return packed\n",
    "\n",
    "    lm_train_dataset = preprocess_dataset(train_dataset)\n",
    "    lm_eval_dataset = preprocess_dataset(eval_dataset)\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        \n",
    "        per_device_train_batch_size=TRAIN_BATCH_SIZE, \n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "        bf16=use_bf16,\n",
    "        fp16=use_fp16,\n",
    "        dataloader_num_workers=8,\n",
    "        optim=\"adamw_torch_fused\",\n",
    "\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\",\n",
    "        greater_is_better=False,\n",
    "        save_total_limit=1,\n",
    "        \n",
    "        report_to=\"none\", \n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "        logging_steps=20,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=lm_train_dataset,\n",
    "        eval_dataset=lm_eval_dataset,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edf1d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(model, tokenizer, prompt_dataset, num_samples=None):\n",
    "    \"\"\"\n",
    "    镜像生成函数\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # --- 1. Tokenizer 设置 ---\n",
    "    # 强制左填充 (Left Padding is crucial for batch generation)\n",
    "    tokenizer.padding_side = \"left\" \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # --- 2. 数据准备与清洗 ---\n",
    "    # 获取原始文本\n",
    "    if num_samples is None:\n",
    "        raw_texts = prompt_dataset[\"text\"]\n",
    "    else:\n",
    "        raw_texts = prompt_dataset[\"text\"][:num_samples]\n",
    "    \n",
    "    # 只保留非空文本，后续逻辑默认数据是干净的\n",
    "    clean_texts = [t for t in raw_texts if len(t.strip()) > 0]\n",
    "    print(f\">>> [Data Clean] Filtered {len(raw_texts)} -> {len(clean_texts)} samples\")\n",
    "\n",
    "    synthetic_texts = []\n",
    "\n",
    "    print(f\">>> [Gen] Generating samples with Batch Size {batch_size}...\")\n",
    "    \n",
    "    # --- 3. 批量生成循环 ---\n",
    "    # 直接遍历 clean_texts，步长为 batch_size\n",
    "    for i in tqdm(range(0, len(clean_texts), batch_size), desc=\"Mirror Gen\"):\n",
    "        batch_prompts = clean_texts[i : i + batch_size]\n",
    "        \n",
    "        # 计算这一批次的目标长度（基于原始文本长度）\n",
    "        # 这里为了计算长度，只进行简单的 tokenize，不占用显存\n",
    "        batch_lens = [len(t) for t in tokenizer(batch_prompts, add_special_tokens=False)[\"input_ids\"]]\n",
    "        \n",
    "        # 动态设定 max_length: 原始最长长度 + 一点冗余，但不超过硬上限\n",
    "        current_max_target = min(max(batch_lens) + 10, max_len)\n",
    "\n",
    "        # 准备输入：截断到 prompt_len (64)\n",
    "        inputs = tokenizer(\n",
    "            batch_prompts, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=prompt_len\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # 使用 BF16 加速 (如果 GPU 支持)\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                outputs = model.generate(\n",
    "                    input_ids=inputs.input_ids,\n",
    "                    attention_mask=inputs.attention_mask,\n",
    "                    max_length=current_max_target, \n",
    "                    do_sample=True,\n",
    "                    top_k=50,\n",
    "                    top_p=0.95,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    use_cache=True \n",
    "                )\n",
    "        \n",
    "        # 解码\n",
    "        gen_texts_batch = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        synthetic_texts.extend(gen_texts_batch)\n",
    "        \n",
    "        # 及时释放显存引用，但【不要】调用 empty_cache()\n",
    "        del inputs, outputs\n",
    "\n",
    "    # --- 4. 恢复环境与最终格式化 ---\n",
    "    tokenizer.padding_side = \"right\" # 恢复默认\n",
    "    \n",
    "    # 去除可能因解码失败产生的空行\n",
    "    final_data = [t for t in synthetic_texts if len(t.strip()) > 0]\n",
    "    print(f\">>> [Post Clean] Final count: {len(final_data)} (Removed {len(synthetic_texts)-len(final_data)} empty/failed)\")\n",
    "\n",
    "    # 返回 Dataset 对象，直接喂给 Trainer\n",
    "    return synthetic_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1e652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsEvaluator:\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        self.device = device\n",
    "        # 使用 GPT-2 Large 作为固定的“上帝视角”特征提取器和裁判\n",
    "        print(\">>> [Metrics] Loading Oracle Model (gpt2-large) for Evaluation...\")\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\").to(device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.real_embeddings_cache = None\n",
    "        self.real_mu = None\n",
    "        self.real_sigma = None\n",
    "\n",
    "    def get_embeddings_and_entropy(self, texts, batch_size=32):\n",
    "        \"\"\"\n",
    "        同时计算 Embeddings (用于FID/OT) 和 Entropy (用于多样性分析)\n",
    "        \"\"\"\n",
    "        all_embeddings = []\n",
    "        all_entropies = []\n",
    "        \n",
    "        # 过滤空文本\n",
    "        texts = [t for t in texts if len(t.strip()) > 0]\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Computing Metrics\", leave=False):\n",
    "            batch_texts = texts[i : i + batch_size]\n",
    "            inputs = self.tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(inputs.input_ids, output_hidden_states=True)\n",
    "                \n",
    "                # --- 1. 获取 Embedding (取最后一层 hidden state 的平均值) ---\n",
    "                # last_hidden_state: [batch, seq_len, hidden_dim]\n",
    "                # mask: [batch, seq_len]\n",
    "                hidden_states = outputs.hidden_states[-1]\n",
    "                mask = inputs.attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "                # Sum / Count (Mean Pooling ignoring padding)\n",
    "                sum_embeddings = torch.sum(hidden_states * mask, 1)\n",
    "                sum_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "                mean_embeddings = sum_embeddings / sum_mask\n",
    "                all_embeddings.append(mean_embeddings.cpu().numpy())\n",
    "\n",
    "                # --- 2. 计算 Entropy ---\n",
    "                # Logits: [batch, seq_len, vocab]\n",
    "                logits = outputs.logits\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                # Shannon Entropy: -sum(p * log(p))\n",
    "                # 我们只关心非Padding部分的熵\n",
    "                token_entropy = -torch.sum(probs * torch.log(probs + 1e-9), dim=-1) # [batch, seq_len]\n",
    "                \n",
    "                # 计算每个句子的平均熵\n",
    "                active_elements = inputs.attention_mask.sum(1)\n",
    "                seq_entropy = (token_entropy * inputs.attention_mask).sum(1) / active_elements\n",
    "                all_entropies.extend(seq_entropy.cpu().tolist())\n",
    "\n",
    "        return np.concatenate(all_embeddings, axis=0), np.mean(all_entropies)\n",
    "\n",
    "    def calculate_frechet_distance(self, mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "        \"\"\"计算 FID / FBD\"\"\"\n",
    "        mu1 = np.atleast_1d(mu1)\n",
    "        mu2 = np.atleast_1d(mu2)\n",
    "        sigma1 = np.atleast_2d(sigma1)\n",
    "        sigma2 = np.atleast_2d(sigma2)\n",
    "\n",
    "        diff = mu1 - mu2\n",
    "\n",
    "        # Product might be almost singular\n",
    "        covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "        if not np.isfinite(covmean).all():\n",
    "            print(\"WARNING: fid calculation produces singular product; adding %s to diagonal of cov estimates\" % eps)\n",
    "            offset = np.eye(sigma1.shape[0]) * eps\n",
    "            covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "\n",
    "        # Numerical error might give slight imaginary component\n",
    "        if np.iscomplexobj(covmean):\n",
    "            if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "                m = np.max(np.abs(covmean.imag))\n",
    "                raise ValueError(\"Imaginary component {}\".format(m))\n",
    "            covmean = covmean.real\n",
    "\n",
    "        tr_covmean = np.trace(covmean)\n",
    "        return (diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean)\n",
    "\n",
    "    def calculate_ot_distance(self, emb1, emb2):\n",
    "        \"\"\"\n",
    "        计算近似 OT 距离 (1-Wasserstein Distance on Embeddings)\n",
    "        这里为了计算速度，计算每一维特征的 Wasserstein 距离的平均值 (Sliced Wasserstein 近似)\n",
    "        \"\"\"\n",
    "        # 如果样本量太大，随机采样 1000 个进行计算以加速\n",
    "        n_sample = min(1000, len(emb1), len(emb2))\n",
    "        idx1 = np.random.choice(len(emb1), n_sample, replace=False)\n",
    "        idx2 = np.random.choice(len(emb2), n_sample, replace=False)\n",
    "        \n",
    "        dists = []\n",
    "        # 对 Embedding 的 1280 维 (gpt2-large) 分别计算分布距离\n",
    "        # 这是一个简化版的 Sliced Wasserstein，如果用精确 OT (Sinkhorn) 会非常慢\n",
    "        for i in range(emb1.shape[1]):\n",
    "            d = wasserstein_distance(emb1[idx1, i], emb2[idx2, i])\n",
    "            dists.append(d)\n",
    "        \n",
    "        return np.mean(dists)\n",
    "\n",
    "    def evaluate_dataset(self, real_texts, synthetic_texts):\n",
    "        # 1. 缓存真实数据的统计量 (只算一次)\n",
    "        if self.real_embeddings_cache is None:\n",
    "            print(\"    | [Metrics] Computing Real Data Embeddings...\")\n",
    "            self.real_embeddings_cache, self.real_entropy = self.get_embeddings_and_entropy(real_texts)\n",
    "            self.real_mu = np.mean(self.real_embeddings_cache, axis=0)\n",
    "            self.real_sigma = np.cov(self.real_embeddings_cache, rowvar=False)\n",
    "\n",
    "        # 2. 计算合成数据统计量\n",
    "        print(\"    | [Metrics] Computing Synthetic Data Embeddings...\")\n",
    "        syn_embeddings, syn_entropy = self.get_embeddings_and_entropy(synthetic_texts)\n",
    "        syn_mu = np.mean(syn_embeddings, axis=0)\n",
    "        syn_sigma = np.cov(syn_embeddings, rowvar=False)\n",
    "\n",
    "        # 3. 计算 FID (FBD)\n",
    "        fid = self.calculate_frechet_distance(self.real_mu, self.real_sigma, syn_mu, syn_sigma)\n",
    "        \n",
    "        # 4. 计算 OT 近似距离\n",
    "        ot_dist = self.calculate_ot_distance(self.real_embeddings_cache, syn_embeddings)\n",
    "\n",
    "        return {\n",
    "            \"fid\": float(fid),\n",
    "            \"ot_distance\": float(ot_dist),\n",
    "            \"syn_entropy\": float(syn_entropy),\n",
    "            \"real_entropy\": float(self.real_entropy) # 作为参考\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
