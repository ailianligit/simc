{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10221d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/simc/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer, \n",
    ")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "DATA_PATH = \"/home/ubuntu/data/dataset/wikitext_dataset\"\n",
    "dataset = load_from_disk(DATA_PATH)\n",
    "train_data = dataset['train']\n",
    "valid_data = dataset['validation']\n",
    "\n",
    "MODEL_PATH = \"/home/ubuntu/data/model/gpt2_model\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_PATH)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f9fcddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (251048 > 1024). Running this sequence through the model will result in indexing errors\n",
      "  0%|          | 0/491 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
      "100%|█████████▉| 489/491 [00:03<00:00, 162.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Perplexity: 26.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def compute_standard_ppl_with_sliding_window(model, tokenizer, dataset, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    结合了：\n",
    "    1. 滑动窗口 (Stride) -> 保证每个 token 都有足够的上文\n",
    "    2. 全局聚合 (Global Aggregation) -> 符合 PPL 标准定义\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. 拼接全量文本\n",
    "    encodings = tokenizer(\"\\n\\n\".join(dataset[\"text\"]), return_tensors=\"pt\")\n",
    "    \n",
    "    max_length = model.config.n_positions\n",
    "    stride = 512  # 滑动步长，通常设为 max_length 的一半或更小\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "    nlls = []\n",
    "    prev_end_loc = 0\n",
    "    \n",
    "    # tqdm 进度条\n",
    "    for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # 我们这一轮实际要评估的 token 长度\n",
    "        \n",
    "        # 获取当前窗口的 input_ids\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        \n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100 \n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            \n",
    "            # outputs.loss 是平均 loss，我们需要还原成 sum loss\n",
    "            # 因为最后一个 batch 的 trg_len 可能不等于 stride\n",
    "            neg_log_likelihood = outputs.loss * trg_len\n",
    "\n",
    "        nlls.append(neg_log_likelihood)\n",
    "        \n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "\n",
    "    # 3. 全局计算 PPL\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / seq_len) # 总 NLL / 总长度\n",
    "    return ppl.item()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "ppl_score = compute_standard_ppl_with_sliding_window(model, tokenizer, valid_data, DEVICE)\n",
    "print(f\"Validation Perplexity: {ppl_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cada79bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1480' max='1480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1480/1480 12:38, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.407000</td>\n",
       "      <td>3.219363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.317700</td>\n",
       "      <td>3.167754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.287000</td>\n",
       "      <td>3.146741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.289100</td>\n",
       "      <td>3.135971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.245700</td>\n",
       "      <td>3.132008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.278700</td>\n",
       "      <td>3.129983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.261700</td>\n",
       "      <td>3.128894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.242500</td>\n",
       "      <td>3.128394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.261200</td>\n",
       "      <td>3.128713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.256200</td>\n",
       "      <td>3.128685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
      "100%|█████████▉| 489/491 [00:03<00:00, 140.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Perplexity: 21.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "MAX_LENGTH = 1024\n",
    "EPOCHS = 10\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "GRADIENT_ACCUMULATION = 1\n",
    "LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 0.05\n",
    "WARMUP_RATIO = 0.03\n",
    "\n",
    "use_bf16 = torch.cuda.is_bf16_supported()\n",
    "use_fp16 = torch.cuda.is_available() and not use_bf16\n",
    "\n",
    "def group_texts(examples):\n",
    "    \"\"\"将文本拼接并切块 (Packing)\"\"\"\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    \n",
    "    if total_length >= MAX_LENGTH:\n",
    "        total_length = (total_length // MAX_LENGTH) * MAX_LENGTH\n",
    "    \n",
    "    result = {\n",
    "        k: [t[i : i + MAX_LENGTH] for i in range(0, total_length, MAX_LENGTH)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "def train_model(model, train_dataset, eval_dataset, tokenizer, output_dir):\n",
    "    \"\"\"\n",
    "    模型微调函数：包含自动列清理、BF16加速、Epoch级评估\n",
    "    \"\"\"\n",
    "    # 内部预处理函数\n",
    "    def preprocess_dataset(dataset):\n",
    "        # 关键：获取当前所有列名 (text, entropy 等)，以便稍后移除\n",
    "        column_names = dataset.column_names\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            # 批量处理：给每个文本末尾加上 EOS\n",
    "            return tokenizer(\n",
    "                [t + tokenizer.eos_token for t in examples[\"text\"]]\n",
    "            )\n",
    "        \n",
    "        # 1. Tokenize 并移除旧列\n",
    "        tokenized = dataset.map(\n",
    "            tokenize_function, \n",
    "            batched=True, \n",
    "            num_proc=8,\n",
    "            remove_columns=column_names # 彻底清理，防止 group_texts 报错\n",
    "        )\n",
    "        \n",
    "        # 2. Packing\n",
    "        packed = tokenized.map(\n",
    "            group_texts, \n",
    "            batched=True,\n",
    "            num_proc=8\n",
    "        )\n",
    "        return packed\n",
    "\n",
    "    lm_train_dataset = preprocess_dataset(train_dataset)\n",
    "    lm_eval_dataset = preprocess_dataset(eval_dataset)\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        \n",
    "        per_device_train_batch_size=TRAIN_BATCH_SIZE, \n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "        bf16=use_bf16,\n",
    "        fp16=use_fp16,\n",
    "        dataloader_num_workers=8,\n",
    "        optim=\"adamw_torch_fused\",\n",
    "\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\",\n",
    "        greater_is_better=False,\n",
    "        save_total_limit=1,\n",
    "        \n",
    "        report_to=\"none\", \n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "        logging_steps=20,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=lm_train_dataset,\n",
    "        eval_dataset=lm_eval_dataset,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    return model\n",
    "\n",
    "real_output_dir = \"model_real_trained\"\n",
    "real_model = train_model(model, train_data, valid_data, tokenizer, real_output_dir)\n",
    "real_model.eval()\n",
    "ppl_score = compute_standard_ppl_with_sliding_window(real_model, tokenizer, valid_data, DEVICE)\n",
    "print(f\"Validation Perplexity: {ppl_score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
